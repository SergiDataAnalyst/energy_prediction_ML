{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gUBH9UHu5Wy"
   },
   "source": [
    "# Energy Prediction Using Machine Learning Algorithms\n",
    "\n",
    "## Index:\n",
    "\n",
    "\n",
    "* ###  1. Introduction\n",
    "* ###  2. Overview and data loading\n",
    "* ###  3. Visualization\n",
    "* ###  4. Missing values and outliers\n",
    "* ###  5. Splitting of the dataset\n",
    "* ###  6. Linear Regression\n",
    "* ###  7. Random Forest Regressor\n",
    "* ###  8. Support Vector Regression\n",
    "* ###  9. Comparison of the results\n",
    "* ### 10. Deployment of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9SfX5PrGcgP"
   },
   "source": [
    "## 1. Introduction \n",
    "\n",
    "In this project I show the implementation of a supervised learning procedure with the necessary pre- and post-processing steps using the case of the energy prediction of a machining process.\n",
    "\n",
    "The transformation of the energy system towards renewable generation, increases the need to manage decentralized flexibilities more efficiently. For this, precise forecasting of uncontrollable electrical load is key. Although there is an abundance of studies presenting innovative individual methods for load forecasting, comprehensive comparisons of popular methods are hard to come across. Overall, identifying a single best method remains a challenge specific to the forecasting task.\n",
    "\n",
    "The goal in this project is to perform a regression analysis using the data that we have to train different regression models to predict the target variable. In our use case we want to predict the energy requirement to perform a milling process.\n",
    "\n",
    "#### 1.1 Motivation for energy prediction\n",
    "\n",
    "1. Creation of transparency and implementation of energy planning\n",
    "2. Adaptation and optimization of the process parameters according to the energy requirement\n",
    "3. Possibility of load management\n",
    "4. Detection of deviations due to the comparison of the prediction and the actual energy profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJEnZwUVJq3A"
   },
   "source": [
    "![energyprediction.png](https://i.imgur.com/1WtJgGY.jpg)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r75JyrV2KZv8"
   },
   "source": [
    "<div>\n",
    "<img src=\"attachment:ML6.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Based on the planned process parameters, the energy required for the milling process is to be forecasted. As a basis for the development of a regression model, tests were carried out on a milling machine to gain sufficient data for the training.\n",
    "\n",
    "#### 1.2 Structure of a milling machine\n",
    "\n",
    "Using the Cartesian coordinate system, a machine can be controlled along each axis. Based on each axis, you typically get the following movements from the perspective of an operator facing the machine:\n",
    "- X axis allows movement “left” and “right”\n",
    "- Y axis allows movement “forward” and “backward”\n",
    "- Z axis allows movement “up” and “down”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvaEfPd8KdHM"
   },
   "source": [
    "![millingmachine.jpeg](https://i0.wp.com/www.autodesk.com/products/fusion-360/blog/wp-content/uploads/2018/07/7-machine-axes.png?resize=450%2C393&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these movements, the right tool and other process parameters (feed, etc.) we can perform the required milling process energy prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLKeW6Wgu5XA"
   },
   "source": [
    "## 2. Data loading and first overview\n",
    "\n",
    "* The given data is stored in a text file containing the following columns,\n",
    "\n",
    "  * Axis X, Y and Z\n",
    "  * Feed [mm/min]\n",
    "  * Path [mm]\n",
    "  * Energy requirement - Target variable [kJ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENm1oLpXu5XC"
   },
   "source": [
    "#### 2.1 Loading the data\n",
    "\n",
    "First we have to load the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qlf0ick3u5XE"
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setting white grid background\n",
    "sns.set_style('whitegrid')\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGwBxS0RyxB2"
   },
   "source": [
    "The next step is to access the prepared data set and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mMhSzIgXu5XH",
    "outputId": "c389ce31-d749-4baf-c3dc-c9b6bc578bc5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'dataset_energy_measurement.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aWlkOnPu5XI"
   },
   "source": [
    "#### 2.1 Overview of the data\n",
    "\n",
    "After that we need an overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets modify the column's titles and add the units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "d90YlpxLu5XJ",
    "outputId": "ef70d5e5-f490-49d7-f8ea-73af2d53887d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed4ryAy7pz1L",
    "outputId": "3f874855-5298-4993-de0f-6aed2e79c8a3"
   },
   "outputs": [],
   "source": [
    "df.shape #this gives us the dimensions of the dataset (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ``describe()`` gives us a quick overview of the dataset values, this can be useful for the initial phases of our pipeline. We can clearly see there are missing values in the columns Axis, Feed and Path, since the count of values does not match with the total number of data entries (226). This is most likely caused by NaN values due to a sensor malfunction or failure to collect data. Furthermore, this method also gives wide information on some basic statistics of the dataset, such as standard deviation, the mean and quartile distribution. Max and Min info are always useful to promptly identify outliers in the data and assess wether to eliminate those from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVEMHrDQu5XT"
   },
   "source": [
    "## 3. Visualization \n",
    "\n",
    "We perform a quick initial visualization of our data in order to better understand the distributions and clusters of data. This will help assessing any potential errors in data collecting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Energy requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Energy_Requirement\"],\n",
    "             kde=True,\n",
    "             kde_kws={\"color\": \"g\", \"alpha\": 0.3, \"linewidth\": 5, \"shade\": True})\n",
    "plt.title(\"Energy requirement distribution\")\n",
    "plt.xlabel(\"Energy requirement (KJ)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "3hhGD-pcu5XY",
    "outputId": "bab460d5-f73a-4824-e0a6-4abe8d3a02e8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Path\"],\n",
    "             kde=True,\n",
    "             kde_kws={\"color\": \"r\", \"alpha\": 0.3, \"linewidth\": 5, \"shade\": True})\n",
    "plt.title(\"Path distribution\")\n",
    "plt.xlabel('Path [mm]')\n",
    "plt.ylabel('Total quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "id": "uT-1t2kzu5XZ",
    "outputId": "ad46704c-efa4-4235-fba2-6ee6ef418ba0"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Axis\"],\n",
    "             kde=True,\n",
    "             kde_kws={\"color\": \"y\", \"alpha\": 0.3, \"linewidth\": 5, \"shade\": True})\n",
    "plt.title(\"Axis distribution\")\n",
    "plt.xlabel('Axis')\n",
    "plt.ylabel('Total quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "EBsfs7PQu5Xa",
    "outputId": "11b65914-12ad-4080-b86e-53f54abd8bfd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Feed\"],\n",
    "             kde=True,\n",
    "             kde_kws={\"color\": \"b\", \"alpha\": 0.4, \"linewidth\": 5, \"shade\": True}, bins=30)\n",
    "plt.title(\"Feed distribution\")\n",
    "plt.xlabel('Feed (mm/min)')\n",
    "plt.ylabel('Total quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmcaZ7AHu5Xb"
   },
   "source": [
    "## 4. Missing values and outliers\n",
    "\n",
    "From the previous distributions we can determine that the `Energy requirement` data has the expected Gaussian distribution, while the rest of the columns are clustered in a different way. The `Path`data is highly clustered with barely any outliers. The `Axis` data clearly has some wrong data since the only desirable inputs should be 1 for axis X, 2 for axis Y and 3 for axis Z. The `Feed` data shows also some minor low inputs which we will exclude from the analysis since they represent a rather small percentage of the total amount of interactions, with doing so we will increase the accuracy in the predictions. \n",
    "\n",
    "Missing values and outliers have to be detected and dealt with in order to prepare the data set for the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp5j5up4u5Xb"
   },
   "source": [
    "#### 4.1 Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values can be NaN values or breaks in the dataset that do not seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "AXkA6SFfu5Xc",
    "outputId": "aaa594f5-a848-4e5d-f605-537c6d1c933c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Ttx2CCbCu5Xd",
    "outputId": "824710fc-1d6c-499d-9830-7ec80c599e81"
   },
   "outputs": [],
   "source": [
    "df=df.dropna() \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbuGuSJtu5Xd"
   },
   "source": [
    "We can see that the rows 5, 6, 7 and 8 have been dropped as they contained some missing values (NaN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KnRjWQgu5Xe"
   },
   "source": [
    "#### 4.2 Handling outliers\n",
    "\n",
    "Before we can deal with outliers, we have to identify them. Outliers are sometimes the result of wrong data or errors in collecting it. Before deciding wether to remove an outlier or not, further inspection on both the dataset and the outlier(s) should be made. Outliers can skew data and mess up with the results(e.g. changing the relationship of a correlation of data). It is for this reason that the nature of the outliers must be understood before removing them.\n",
    "\n",
    "Since we have performed the tests for the independent variables (feed, axis and path) we know the range of these values. \n",
    "1. Axis: 1 to 3\n",
    "2. distance: -60 to 60 [mm]\n",
    "3. Feed rate: 500 to 3000 [mm/min]\n",
    "\n",
    "All values outside these ranges are therefore outliers. The relevant instances should therefore be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of features outside the range of the are removed and we will only include those values of the feature that lie in the particular ranges of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GS9b2lLu5Xi",
    "outputId": "caef1e1d-eec3-4f06-a34a-d4411f892d7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[(df.Axis >= 1) & (df.Axis <= 3) & \n",
    "            (df.Path >= -60) & (df.Path <= 60) &\n",
    "            (df.Feed >= 500) & (df.Feed <= 3000)\n",
    "            ]  \n",
    "df.shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the ouliers we analyze the data set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "MbJcFYyBu5Xi",
    "outputId": "964f8bd2-3cd2-4c42-ee29-85b31852ae9f"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now observe a change in the parameters of the distributions of the remaining data. The mean values have remained fairly equal for the `Axis` and `Feed` columns but it has improved considerably for the `Path` column. In terms of the standard deviation, all three have been reduced, which means the data is now more densily clustered. We have reduced the size of our dataset from 220 to 218 counts, which doesn't involve a great loss of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdBJDZlqu5Xk"
   },
   "source": [
    "## 5. Splitting of the data set\n",
    "\n",
    "Before we continue with the training, we have to split our data set.\n",
    "\n",
    "1. Training dataset: the training dataset is used to determine the models parameters based on the data it has seen. Here, the labels are provided to the model so that it can learn about potential patterns in the data and thus adjust its parameters in such a way that it can predict data points.\n",
    "\n",
    "2. Test dataset: This dataset is used to test the performance of the model. It can be used to see if the model is able to perform well on data which it has never seen before.\n",
    "\n",
    "3. Target variable: Further we have to separate the target variable from the other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppi0r6oeu5Xl"
   },
   "source": [
    "#### 5.1 Seperation features and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the features (axis, feed, distance) and store it in 'X_multi' then we store our target varible (energy) in 'Y_target' to train our model. Energy requirement is our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiwzx6gYu5Xl",
    "outputId": "c57803a8-8d82-4dc6-b095-2a07f9eb74c1"
   },
   "outputs": [],
   "source": [
    "X_multi = df.drop('Energy_Requirement', 1)\n",
    "\n",
    "Y_target = df.Energy_Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubIoR0rLu5Xl"
   },
   "source": [
    "#### 5.2 Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given dataset is divided into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abdiakFcu5Xm"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_multi, Y_target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shapes of the datasets so that we dont wrongly fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pO3UMVhu5Xm",
    "outputId": "53010284-1937-4757-bef4-dde3c9366938"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation the default value for the test size is set to complement the train size. In case there is no value assigned to the train size, a default value of 0.25 is set for the test size, meaning the dataset will be split in 75% training data and 25% testing data. The train and test sizes are usually set to be between 70-30 % and 90-10 %. Choosing the size depends on wether the size of the data is already representative or not. Small training datasets may incur in larger error for the predictions. Generally speaking we want the training dataset to be larger than the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5Wn0uWru5Xq"
   },
   "source": [
    "## 6. Linear Regression\n",
    "\n",
    "Now we can start to use machine learning algorithms to predict the required energy. For that we carry out the following steps: \n",
    "\n",
    "1. We import a Linear Regression algorithm from the Sklearn library.\n",
    "2. The `fit()` function of the Linear Regression model is used to train our model with the training dataset. \n",
    "3. The `predict()` function is used to make predictions on a given dataset with the trained model.\n",
    "4. Calculation of the losses to assess the performance.\n",
    "5. Visualisation ot the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression is a type of regression analysis where the number of independent variables is one and there is a linear relationship between the independent(x) and dependent(y) variable. A two-dimensional linear regression equation could be simplified as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://i.imgur.com/bOR782v.png\" width=\"110\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression algorithms from sklearn follow a gradient descent approach in which the objective is to find a minimum for the mean squared error (MSE) function. All MSE functions for linear regression are always convex, thus no need to adjust the learning rate. The cost function helps us to figure out the best possible values for a_0 and a_1 which would provide the best fit line for the data points. Since we want the best values for a_0 and a_1, we convert this search problem into a minimization problem where we would like to minimize the error between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://i.imgur.com/qMvSVTq.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update a_0 and a_1, we take gradients from the cost function. To find these gradients, we take partial derivatives with respect to a_0 and a_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://i.imgur.com/2q9oYeC.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-_dlqapu5Xq"
   },
   "source": [
    "#### 6.1 Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euyvh3dTu5Xq"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD-Ril2pu5Xr"
   },
   "source": [
    "#### 6.2 Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0dojdT0u5Xr",
    "outputId": "a08a617e-b3f9-40cc-cb41-dd4af5c7f9d1"
   },
   "outputs": [],
   "source": [
    "lreg = LinearRegression()\n",
    "\n",
    "lreg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZkxCWL6u5Xs"
   },
   "source": [
    "#### 6.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the prediction of the training data and the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXZV5b3ku5Xt"
   },
   "outputs": [],
   "source": [
    "pred_train = lreg.predict(X_train) # prediction of the training data\n",
    "pred_test = lreg.predict(X_test) # prediction of unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZq6uOq5u5Xt"
   },
   "source": [
    "#### 6.4 Calculate the different losses (Least Square Error, Mean Square Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error and mean squared error are both used as ways to measure an overall error between the predictions and the actual values. MEA is commonly used when having a small number of outliers or when they present a small percentage of the total data. MEA is also used when the predictions are not aimed to fit the outliers. \n",
    "MSA on the other hand punishes the existance of outliers and it is a good method if we want our predictions to better suited to the outliers. We now calulate the error for the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Red0IB3u5Xt",
    "outputId": "7c95e03e-65f0-46ec-b09a-19e061232337",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Training data\n",
    "MSE_linear_Train_Data = mean_squared_error(Y_train, pred_train)\n",
    "MAE_linear_Train_Data = mean_absolute_error(Y_train, pred_train)\n",
    "\n",
    "print(\"The Mean Square Error on the training data is:\", MSE_linear_Train_Data)\n",
    "print(\"The Mean Absolute Error on the training data is:\", MAE_linear_Train_Data)\n",
    "\n",
    "# Test data / unseen data\n",
    "MSE_linear_Test_Data = mean_squared_error(Y_test, pred_test)\n",
    "MAE_linear_Test_Data = mean_absolute_error(Y_test, pred_test)\n",
    "\n",
    "print(\"The Mean Square Error on the test data is:\", MSE_linear_Test_Data)\n",
    "print(\"The Mean Absolute Error on the test data is:\", MAE_linear_Test_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzqUXH99u5Xu"
   },
   "source": [
    "The errors are small in both cases, we can appreciate a lower MSE value for both, the training and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfMNjqr0u5Xv"
   },
   "source": [
    "#### 6.5 Residual plots for Linear Regression\n",
    "\n",
    "1. A residual value is a measure of how much a regression line vertically misses a data point.\n",
    "2. In a residual plot the residual values are on the vertical axis and the horizontal axis displays the independent variable.\n",
    "3. Ideally, residual values should be equally and randomly distributed around the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "t_8npb0eu5X3",
    "outputId": "bd03ab3e-9b3e-4f7e-e126-41a70e0fd9b7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We want the data points to be scattered around the horizontal line\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,7))\n",
    "train = plt.scatter(pred_train, (pred_train-Y_train), c='b', alpha=0.5)\n",
    "test = plt.scatter(pred_test, (pred_test-Y_test), c='r', alpha=0.5)\n",
    "plt.hlines(y=0, xmin=-0.5, xmax=0.5)\n",
    "plt.legend((train, test), ('Training', 'Test'),loc='lower left')\n",
    "plt.title('Residual plot for Linear Regressor')\n",
    "plt.xlabel(\"Energy_Requirement - Target variable\")\n",
    "plt.ylabel(\"Residual Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual values should be equally and randomly distributed along the horizontal axis. The occurrence of patterns and noncentral clustering rules out the linear regression model as a candidate to evaluate and predict our energy requirement data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtcvXIVru5X4"
   },
   "source": [
    "## 7. Random Forest Regressor\n",
    "\n",
    "This is another type of regression algorithm that uses averaging to improve predictive accuracy (summed up over the number of trees) and controls overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8lRHHwgu5X4"
   },
   "source": [
    "#### 7.1 Import the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate model with 1000 decision trees and a random state of 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di14-Ayau5X4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV3Hikomu5X5"
   },
   "source": [
    "Meaning of the hyperparameters used above,\n",
    "\n",
    "1. `n_estimators`: This is the number of trees to build before taking the maximum voting or averages of predictions. Higher number of trees can increase the performance but make ourr code slower. For our dataset, 1000 is a good choice.\n",
    "\n",
    "2. `random_state`: This parameter makes a solution easy to replicate. A definite value of random_state will always produce same results if given with same parameters and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBuWp-JAu5X5"
   },
   "source": [
    "#### 7.2 Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCerYx2gu5X5",
    "outputId": "243fc5b6-8ecd-4999-9071-2b85100f1aac"
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J8tXYeAu5X6"
   },
   "source": [
    "#### 7.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the forest's predict method on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kenCaGZDu5X6"
   },
   "outputs": [],
   "source": [
    "rf_pred_train = rf.predict(X_train)\n",
    "rf_pred_test = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8caZkKwu5X6"
   },
   "source": [
    "#### 7.4 Calculate the different losses (Mean Absolute Error, Mean Square Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calulate the error for the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fnIY7TQu5X7",
    "outputId": "47796491-652b-43ed-b19b-3157e2be765c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Training data\n",
    "MSE_rf_Train_Data = mean_squared_error(Y_train, rf_pred_train)\n",
    "MAE_rf_Train_Data = mean_absolute_error(Y_train, rf_pred_train)\n",
    "\n",
    "print(\"Mean Square Error on the training data is:\", MSE_rf_Train_Data)\n",
    "print(\"Mean Absolute Error on the training data is:\", MAE_rf_Train_Data)\n",
    "\n",
    "# Unseen test data\n",
    "MSE_rf_Test_Data = mean_squared_error(Y_test, rf_pred_test)\n",
    "MAE_rf_Test_Data = mean_absolute_error(Y_test, rf_pred_test)\n",
    "\n",
    "print(\"\\n\"\"Mean Square Error on the test data is:\", MSE_rf_Test_Data)\n",
    "print(\"Mean Absolute Error on the test data is:\", MAE_rf_Test_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are far better than the previous obtained results with the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62wVZqZnu5X7"
   },
   "source": [
    "\n",
    "#### 7.5 Residual plots for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would want the data points to be scattered around the horizontal line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "dtDxaLHDu5X7",
    "outputId": "4cf136cf-724f-44ff-d5c0-e646d58920af",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,7))\n",
    "train = plt.scatter(rf_pred_train, (rf_pred_train-Y_train), c='b', alpha=0.5)\n",
    "test = plt.scatter(rf_pred_test, (rf_pred_test-Y_test), c='r', alpha=0.5)\n",
    "plt.hlines(y=0, xmin=-0.5, xmax=0.5)\n",
    "\n",
    "plt.legend((train, test), ('Training', 'Test'),loc='lower left')\n",
    "plt.title(\"Residual plot for random forest\")\n",
    "plt.xlabel(\"Energy_Requirement - Target variable\")\n",
    "plt.ylabel(\"Residual Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96x3-aX2u5X7"
   },
   "source": [
    "Here we see that the model performs well on both, the training dataset as well as the test dataset, as the blue and red points are fairly close to the horizontal line and they are equally distributed. A fairly notable horizontal symmetry is observed, as well as a clustering towards the center. All this confirms the random forest regressor as a good model to predict our energy requirement data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epBfrUrVu5X8"
   },
   "source": [
    "## 8. Support Vector Regression (SVR)\n",
    "\n",
    "1. The SVR tries to best approximate a line beween the features in order to predict the target variable.\n",
    "2. This type of regressor can be used for linear and non-linear regression problems i.e. the best fitting function can be non-linear as well. The kernel functions transform the data into a higher dimensional feature space to make it possible to perform the linear separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oW06pkF0u5X8"
   },
   "source": [
    "#### 8.1 Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8g3d8A26u5X8"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy1PMHeZu5X9"
   },
   "source": [
    "#### 8.2 Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nep0c5KIu5X9",
    "outputId": "015bc944-44e6-4133-c1a0-cbbc3a590c0b"
   },
   "outputs": [],
   "source": [
    "regressor = SVR(kernel = 'rbf')\n",
    "regressor.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYKCCqcbu5X9"
   },
   "source": [
    "#### 8.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the values for the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cU-aSuyLu5X-"
   },
   "outputs": [],
   "source": [
    "pred_train = regressor.predict(X_train) \n",
    "pred_test = regressor.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSB7uu2pu5X-"
   },
   "source": [
    "#### 8.4 Calculate the different losses (Mean Absolute Error, Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFE6V2cDu5X-",
    "outputId": "d5e6239f-8c7e-463f-89ca-d5d3f2f2eec1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Training data\n",
    "MSE_svr_Train_Data = mean_squared_error(Y_train, pred_train)\n",
    "MAE_svr_Train_Data = mean_absolute_error(Y_train, pred_train)\n",
    "\n",
    "print(\"The Mean Square Error on the training data is:\", MSE_linear_Train_Data)\n",
    "print(\"The Mean Absolute Error on the training data is:\", MAE_linear_Train_Data)\n",
    "\n",
    "# Test data / unseen data\n",
    "MSE_svr_Test_Data = mean_squared_error(Y_test, pred_test)\n",
    "MAE_svr_Test_Data = mean_absolute_error(Y_test, pred_test)\n",
    "\n",
    "print(\"The Mean Square Error on the test data is:\", MSE_linear_Test_Data)\n",
    "print(\"The Mean Absolute Error on the test data is:\", MAE_linear_Test_Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UE_NXeu5X-"
   },
   "source": [
    "#### 8.5 Residual plots for Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "Q0_lPUU-u5X_",
    "outputId": "a47fea59-d6d2-498c-f0d0-69219f918860",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,7))\n",
    "train = plt.scatter(pred_train, (pred_train-Y_train), c='b', alpha=0.5)\n",
    "test = plt.scatter(pred_test, (pred_test-Y_test), c='r', alpha=0.5)\n",
    "plt.hlines(y=0, xmin=-0.5, xmax=0.5)\n",
    "plt.legend((train, test), ('Training', 'Test'),loc='lower left')\n",
    "plt.title('Residual plot for SVR')\n",
    "plt.xlabel(\"Energy_Requirement - Target variable\")\n",
    "plt.ylabel(\"Residual Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the linear regression model, we find this model does not qualify to be a good predictor for the energy requirement data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_E47TBsu5X_"
   },
   "source": [
    "## 9. Comparison and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "JtM73jg7u5YA",
    "outputId": "25294587-24c9-40e8-f858-319b48bb1bce"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(['MSE_LR'],[MSE_linear_Test_Data], color=['#4DBEEE'], label=\"Mean Square Error on Linear Regressor\")\n",
    "plt.bar(['MSE_SVR'],[MSE_svr_Test_Data], color=['#A2142F'], label=\"Mean Square Error on SVR\")\n",
    "plt.bar(['MSE_RF'],[MSE_rf_Test_Data], color=['#0072BD'], label=\"Mean Square Error on Random Forest\")\n",
    "\n",
    "plt.bar(['MAE_LR'],[MAE_linear_Test_Data], color=['#D95319'], label=\"Mean Absolute Error on Linear Regressor\")\n",
    "plt.bar(['MAE_SVR'],[MAE_svr_Test_Data], color=['#EDB120'], label=\"Mean Absolute Error on SVR\")\n",
    "plt.bar(['MAE_RF'],[MAE_rf_Test_Data], color=['#77AC30'], label=\"Mean Absolute Error on Random Forest\")\n",
    "\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Error values')\n",
    "plt.title('Performance of different regression models on the test data')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqrX7AE1u5YD"
   },
   "source": [
    "We can observe how the different models perform on our dataset. Taking into account the MSE and the MAE, the SVR has the lowest performance on our model, followed close by the LR model. The results show that the RF model has the best results. The MAE for all methods is larger than the MSE, this is most likely caused by the removal of outliers before the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvIUe7-uu5YE"
   },
   "source": [
    "## 10. Deployment of the model\n",
    "\n",
    "Now lets predict the energy requirement for the following settings of production parameters with the Random Forest model.\n",
    "\n",
    " 1. Setting 1: axis = 2, feed = 800 [mm/min], distance = 60 [mm]\n",
    " 2. Setting 2: axis = 3, feed = 2000 [mm/min], distance = 40 [mm]\n",
    " 3. Setting 3: axis = 1, feed = 1200 [mm/min], distance = -20 [mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65pFuZzcu5YF",
    "outputId": "6a96a1b6-84c3-41e2-f35e-818fdd5da444"
   },
   "outputs": [],
   "source": [
    "settings = ([2, 800, 60], [3, 2000, 40], [1, 1200, -20])\n",
    "\n",
    "# predicting the outcome for the values in settings\n",
    "predicted_energy = rf.predict(settings)\n",
    "\n",
    "# Since predicted_energy is a 1D array we can access each setting with the following code\n",
    "print('Predicted Energy Requirement for setting 1 is ' + str(predicted_energy[0]) + \" KJ\")\n",
    "print('Predicted Energy Requirement for setting 2 is ' + str(predicted_energy[1]) + \" KJ\")\n",
    "print('Predicted Energy Requirement for setting 3 is ' + str(predicted_energy[2]) + \" KJ\")\n",
    "\n",
    "# The sum of the predicted energy predictions for the 3 settings\n",
    "print('Predicted Energy Requirement for all settings is ' + str(predicted_energy.sum()) + \" KJ\")  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
